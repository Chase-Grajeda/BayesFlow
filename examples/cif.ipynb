{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from bayesflow.networks import CouplingFlow\n",
    "from bayesflow.networks import InferenceNetwork\n",
    "from bayesflow.distributions import DiagonalNormal\n",
    "from bayesflow import Approximator\n",
    "from bayesflow import OfflineDataset\n",
    "from bayesflow import OnlineDataset\n",
    "from bayesflow.simulators import TwoMoonsSimulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find example datasets here: https://github.com/jrmcornish/cif/blob/master/cif/datasets/two_d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "['r', 'alpha', 'theta', 'x']\n"
     ]
    }
   ],
   "source": [
    "# def sample_2lines(batch_size, num_batches):\n",
    "#     samples = batch_size * num_batches\n",
    "#     x1 = np.empty(samples)\n",
    "#     x1[:samples//2] = -1.\n",
    "#     x1[samples//2:] = 1.\n",
    "#     x1 += 0.01 * (np.random.rand(samples) - .5)\n",
    "#     x2 = 2 * (np.random.rand(samples) - 0.5)\n",
    "#     return dict(x1=x1, x2=x2)\n",
    "\n",
    "# def extract_params(x: dict):\n",
    "#     z = np.stack([x[key] for key in x.keys()])\n",
    "#     return z\n",
    "\n",
    "batch_size = 128\n",
    "num_batches = 32\n",
    "# data = sample_2lines(batch_size, num_batches)\n",
    "\n",
    "simulator = TwoMoonsSimulator()\n",
    "data = simulator.sample((batch_size * num_batches,))\n",
    "dataset = OfflineDataset(data, workers=4, batch_size=batch_size)\n",
    "print(\"Batch size:\", dataset.batch_size)\n",
    "print([key for key in dataset[0].keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CINF2(InferenceNetwork):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(base_distribution=\"normal\", **kwargs)\n",
    "        # Member variables according to paper\n",
    "        # prior is base dist\n",
    "        self.p_u_density = CouplingFlow()\n",
    "        self.q_u_density = CouplingFlow()\n",
    "        \n",
    "        self.feature_net = CouplingFlow()\n",
    "    \n",
    "    def build(self, xz_shape, conditions_shape):\n",
    "        super().build(xz_shape)\n",
    "        \n",
    "        xz = keras.ops.zeros(xz_shape)\n",
    "        # xz = keras.random.beta(xz_shape, 2, 2)\n",
    "        if conditions_shape is None:\n",
    "            conditions = None\n",
    "        else:\n",
    "            conditions = keras.ops.zeros(conditions_shape)\n",
    "            # conditions = keras.random.beta(conditions_shape, 2, 2)\n",
    "            \n",
    "        \n",
    "        # Build local layers and couplings\n",
    "        self.p_u_density.build(xz_shape, xz_shape)\n",
    "        self.q_u_density.build(xz_shape, conditions_shape)\n",
    "        self.feature_net.build(xz_shape, conditions_shape)\n",
    "        \n",
    "        \n",
    "        self.call(xz, conditions)\n",
    "    \n",
    "    def call(self, xz, conditions):\n",
    "        return self._forward(xz, conditions)\n",
    "    \n",
    "    def bijection(self, x):\n",
    "        # TODO: Make conditional\n",
    "        # z = keras.ops.log(x) - keras.ops.log(1-x)\n",
    "        z = keras.ops.sigmoid(x)\n",
    "        \n",
    "        # Log-jacobian\n",
    "        eps = 1e-7\n",
    "        x_clipped = keras.ops.clip(x, eps, 1-eps)\n",
    "        z_log_clipped = -keras.ops.log(x_clipped) - keras.ops.log(1-x_clipped)\n",
    "        z_log_jac = -keras.ops.sum(z_log_clipped, axis=1, keepdims=True)\n",
    "        z_log_jac = keras.ops.squeeze(z_log_jac)\n",
    "        \n",
    "        return z, z_log_jac\n",
    "    \n",
    "    def _forward(self, x, conditions):\n",
    "        return self.elbo(x, conditions)\n",
    "    \n",
    "    \n",
    "    def _inverse(self, x, conditions):\n",
    "        return self.elbo(x, conditions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def elbo(self, x, conditions):\n",
    "        # sample_shape = 1 if x.shape[0] is None else x.shape[0]\n",
    "        # fix_this = 128 # needs to somehow infer batch size generally\n",
    "        print(x.shape)\n",
    "        fix_this = keras.ops.shape(x)[0]\n",
    "        u = self.q_u_density.sample((128,), conditions=conditions)\n",
    "        log_q_u = self.q_u_density.log_prob(u, conditions=conditions)\n",
    "        \n",
    "        # bijection sampling\n",
    "        z, z_log_jac = self.bijection(x)\n",
    "        \n",
    "        # p_u sampling\n",
    "        log_p_u = self.p_u_density.log_prob(u, conditions=z)\n",
    "        \n",
    "        # prior sampling\n",
    "        log_prior = self.base_distribution.log_prob(z)\n",
    "        \n",
    "        # elbo\n",
    "        log_p = z_log_jac + log_p_u + log_prior\n",
    "        log_q = log_q_u # missing prior elbo call, which is just zeros\n",
    "        log_density = log_p - log_q\n",
    "        \n",
    "        return z, log_density\n",
    "    \n",
    "    \n",
    "    def compute_metrics(self, data, stage=\"training\"):\n",
    "        base_metrics = super().compute_metrics(data, stage=stage)\n",
    "        inference_variables = data[\"inference_variables\"]\n",
    "        inference_conditions = data.get(\"inference_conditions\")\n",
    "        \n",
    "        z, log_density = self(inference_variables, conditions=inference_conditions)\n",
    "        loss = -keras.ops.mean(log_density)\n",
    "        return base_metrics | {\"loss\": loss}\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CINF(InferenceNetwork):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(base_distribution=\"normal\", **kwargs)\n",
    "        # Member variables according to nux implementation\n",
    "        self.feature_net = CouplingFlow() \t # no conditions\n",
    "        self.flow = CouplingFlow() \t\t\t # bijective transformer\n",
    "        self.u_dist = self.base_distribution # Gaussian prior\n",
    "        self.v_dist = CouplingFlow()\t\t # conditioned flow / parameterized prior\n",
    "        \n",
    "    \n",
    "    def build(self, xz_shape, conditions_shape):\n",
    "        super().build(xz_shape)            \n",
    "        self.feature_net.build(xz_shape)\n",
    "        self.flow.build(xz_shape, xz_shape)\n",
    "        self.v_dist.build(xz_shape, xz_shape)\n",
    "        \n",
    "    \n",
    "    def call(self, xz, conditions, inverse=False, **kwargs):\n",
    "        if inverse:\n",
    "            return self._inverse(xz, conditions, **kwargs)\n",
    "        return self._forward(xz, conditions, **kwargs)\n",
    "    \n",
    "    \n",
    "    def _forward(self, x, conditions, density=False, **kwargs):\n",
    "        # Sample u ~ q(u|phi_x)\n",
    "        phi_x = self.feature_net(x, conditions=None)\n",
    "        u, log_qu = self.v_dist(keras.ops.zeros_like(x), conditions=phi_x, inverse=True, density=True)\n",
    "        \n",
    "        # Compute z = f(x; phi_u) and p(x|u)\n",
    "        phi_u = self.feature_net(u, conditions=None)\n",
    "        z, log_px = self.flow(x, conditions=phi_u, inverse=False, density=True)\n",
    "        \n",
    "        # Compute p(u)\n",
    "        log_pu = self.base_distribution.log_prob(u)\n",
    "        \n",
    "        # Log likelihood?\n",
    "        llc = log_px + log_pu - log_qu\n",
    "        \n",
    "        # NOTE - this can be moved up when I'm done tinkering\n",
    "        if density:\n",
    "            return z, llc\n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def _inverse(self, z, conditions, density=False, **kwargs):\n",
    "        # Sample u ~ p(u)\n",
    "        u = self.base_distribution.sample(keras.ops.shape(z))\n",
    "        log_pu = self.base_distribution.log_prob(keras.ops.zeros_like(z))\n",
    "        \n",
    "        # Compute inverse of f(z; u)\n",
    "        phi_u = self.feature_net(u)\n",
    "        x, log_px = self.flow(z, conditions=phi_u, inverse=True, density=True)\n",
    "        \n",
    "        # Predict q(u|x)\n",
    "        phi_x = self.feature_net(x)\n",
    "        _, log_qu = self.v_dist(u, conditions=phi_x, inverse=False, density=True)\n",
    "        \n",
    "        # Log likelihood?\n",
    "        llc = log_px + log_pu - log_qu\n",
    "        \n",
    "        # NOTE: this can be moved up when I'm done tinkering\n",
    "        if density:\n",
    "            return x, llc\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def compute_metrics(self, data, stage=\"training\"):\n",
    "        base_metrics = super().compute_metrics(data, stage=stage)\n",
    "        inference_variables = data[\"inference_variables\"]\n",
    "        inference_conditions = data.get(\"inference_conditions\")\n",
    "        \n",
    "        z, log_density = self(inference_variables, conditions=inference_conditions, inverse=False, density=True)\n",
    "        loss = -keras.ops.mean(log_density)\n",
    "        return base_metrics | {\"loss\": loss}\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinf = CINF()\n",
    "approximator = Approximator(\n",
    "    inference_network=cinf,\n",
    "    inference_variables=[\"theta\"],\n",
    "    inference_conditions=[\"r\", \"alpha\", \"x\"]\n",
    ")\n",
    "approximator.compile(optimizer=\"adamw\", loss=\"mse\")\n",
    "approximator.build_from_data(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 110ms/step - loss: nan\n",
      "Epoch 2/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13b3c1a93a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics = approximator.evaluate(dataset, return_dict=True)\n",
    "approximator.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling SingleCoupling.call().\n\n\u001b[1m{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [128,2,1] vs. shape[1] = [128,2,2,2] [Op:ConcatV2] name: concat\u001b[0m\n\nArguments received by SingleCoupling.call():\n  • x1=tf.Tensor(shape=(128, 2, 1), dtype=float32)\n  • x2=tf.Tensor(shape=(128, 2, 1), dtype=float32)\n  • conditions=tf.Tensor(shape=(128, 2, 2, 2), dtype=float32)\n  • inverse=True\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m approximator\u001b[38;5;241m.\u001b[39msample((\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset)))\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\approximators\\base_approximator.py:49\u001b[0m, in \u001b[0;36mBaseApproximator.sample\u001b[1;34m(self, batch_shape, data, numpy)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inference_conditions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mndim(inference_conditions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     47\u001b[0m     inference_conditions \u001b[38;5;241m=\u001b[39m expand_tile(inference_conditions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n\u001b[38;5;241m=\u001b[39mnum_samples)\n\u001b[1;32m---> 49\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_network\u001b[38;5;241m.\u001b[39msample(batch_shape, conditions\u001b[38;5;241m=\u001b[39minference_conditions)\n\u001b[0;32m     50\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigurator\u001b[38;5;241m.\u001b[39mdeconfigure(samples)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_network \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\networks\\inference_network.py:31\u001b[0m, in \u001b[0;36mInferenceNetwork.sample\u001b[1;34m(self, batch_shape, conditions, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_shape: Shape, conditions: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     30\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_distribution\u001b[38;5;241m.\u001b[39msample(batch_shape)\n\u001b[1;32m---> 31\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(samples, conditions\u001b[38;5;241m=\u001b[39mconditions, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[1;32mc:\\Users\\grajec\\miniconda3\\envs\\bayesflow\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mCINF.call\u001b[1;34m(self, xz, conditions, inverse, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, xz, conditions, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[1;32m---> 20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverse(xz, conditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(xz, conditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m, in \u001b[0;36mCINF._inverse\u001b[1;34m(self, z, conditions, density, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Compute inverse of f(z; u)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m phi_u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_net(u)\n\u001b[1;32m---> 52\u001b[0m x, log_px \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow(z, conditions\u001b[38;5;241m=\u001b[39mphi_u, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Predict q(u|x)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m phi_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_net(x)\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\networks\\coupling_flow\\coupling_flow.py:72\u001b[0m, in \u001b[0;36mCouplingFlow.call\u001b[1;34m(self, xz, conditions, inverse, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m, xz: Tensor, conditions: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, inverse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     70\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor]:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverse(xz, conditions\u001b[38;5;241m=\u001b[39mconditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(xz, conditions\u001b[38;5;241m=\u001b[39mconditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\networks\\coupling_flow\\coupling_flow.py:97\u001b[0m, in \u001b[0;36mCouplingFlow._inverse\u001b[1;34m(self, z, conditions, density, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m log_det \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mzeros(keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mshape(z)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvertible_layers):\n\u001b[1;32m---> 97\u001b[0m     x, det \u001b[38;5;241m=\u001b[39m layer(x, conditions\u001b[38;5;241m=\u001b[39mconditions, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m     log_det \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m det\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m density:\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\networks\\coupling_flow\\couplings\\dual_coupling.py:33\u001b[0m, in \u001b[0;36mDualCoupling.call\u001b[1;34m(self, xz, conditions, inverse, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, xz: Tensor, conditions: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, inverse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (Tensor, Tensor):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[1;32m---> 33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverse(xz, conditions\u001b[38;5;241m=\u001b[39mconditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(xz, conditions\u001b[38;5;241m=\u001b[39mconditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\networks\\coupling_flow\\couplings\\dual_coupling.py:50\u001b[0m, in \u001b[0;36mDualCoupling._inverse\u001b[1;34m(self, z, conditions, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform (g(x1; f(x2; x1)), f(x2; x1)) -> (x1, x2)\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m z1, z2 \u001b[38;5;241m=\u001b[39m z[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpivot], z[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpivot :]\n\u001b[1;32m---> 50\u001b[0m (z2, z1), log_det2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupling2(z2, z1, conditions\u001b[38;5;241m=\u001b[39mconditions, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     51\u001b[0m (x1, x2), log_det1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupling1(z1, z2, conditions\u001b[38;5;241m=\u001b[39mconditions, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconcatenate([x1, x2], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\networks\\coupling_flow\\couplings\\single_coupling.py:46\u001b[0m, in \u001b[0;36mSingleCoupling.call\u001b[1;34m(self, x1, x2, conditions, inverse, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m, x1: Tensor, x2: Tensor, conditions: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, inverse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     44\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ((Tensor, Tensor), Tensor):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[1;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverse(x1, x2, conditions\u001b[38;5;241m=\u001b[39mconditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(x1, x2, conditions\u001b[38;5;241m=\u001b[39mconditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\networks\\coupling_flow\\couplings\\single_coupling.py:60\u001b[0m, in \u001b[0;36mSingleCoupling._inverse\u001b[1;34m(self, z1, z2, conditions, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform (x1, f(x2; x1)) -> (x1, x2)\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m x1 \u001b[38;5;241m=\u001b[39m z1\n\u001b[1;32m---> 60\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parameters(x1, conditions\u001b[38;5;241m=\u001b[39mconditions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     61\u001b[0m x2, log_det \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(z2, parameters\u001b[38;5;241m=\u001b[39mparameters, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (x1, x2), log_det\n",
      "File \u001b[1;32mc:\\Users\\grajec\\Desktop\\Code Projects\\BayesFlow\\BayesFlow\\examples\\..\\bayesflow\\networks\\coupling_flow\\couplings\\single_coupling.py:67\u001b[0m, in \u001b[0;36mSingleCoupling.get_parameters\u001b[1;34m(self, x, conditions, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, conditions: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conditions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m         x \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconcatenate([x, conditions], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_projector(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m     70\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39msplit_parameters(parameters)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling SingleCoupling.call().\n\n\u001b[1m{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [128,2,1] vs. shape[1] = [128,2,2,2] [Op:ConcatV2] name: concat\u001b[0m\n\nArguments received by SingleCoupling.call():\n  • x1=tf.Tensor(shape=(128, 2, 1), dtype=float32)\n  • x2=tf.Tensor(shape=(128, 2, 1), dtype=float32)\n  • conditions=tf.Tensor(shape=(128, 2, 2, 2), dtype=float32)\n  • inverse=True\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "samples = approximator.sample((128,2), next(iter(dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
